{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner version 1.1\n",
    "# Date - 10/3/2023\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "all_files_text = []\n",
    "\n",
    "# Grabbing each article for each group ex. all the articles in group 1. \n",
    "# Just have to change the folder name for each group.\n",
    "files_path = [file for file in glob.glob(r'articles\\*_*.txt') if os.path.isfile(file)]\n",
    "\n",
    "\n",
    "\n",
    "# Find what file is having decoding issues\n",
    "# If there is a url, check to see if the character is important\n",
    "# Otherwise we'll just use replace to get rid of the character\n",
    "# for file in files_path:\n",
    "#    try:\n",
    "#            all_files_text.append(f.read())\n",
    "#    except UnicodeDecodeError:\n",
    "#        print(f\"Error decoding file: {file}\")\n",
    "\n",
    "# Read each file and append the text to a list\n",
    "for file in files_path:\n",
    "    with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "       all_files_text.append(f.read())\n",
    "\n",
    "all_files_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git rid of garabage characters\n",
    "text = [x.replace('\\n', ' ') for x in all_files_text]\n",
    "text = [x.replace('\\t', ' ') for x in text]\n",
    "\n",
    "# Set the text to lower case\n",
    "text = [x.lower() for x in text]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "text = [word_tokenize(x) for x in text]\n",
    "\n",
    "# tag the text\n",
    "tagged_text = [pos_tag(x) for x in text]\n",
    "\n",
    "tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = []\n",
    "\n",
    "\n",
    "# Update the part of speech tags to be compatible with the lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "for sentence in tagged_text:\n",
    "    lemmatized_text.append([lemmatizer.lemmatize(word[0], get_wordnet_pos(word[1])) for word in sentence])\n",
    "\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stop list array\n",
    "stops = list(set(stopwords.words('english'))) + [',', '.' , '-', 'however', 'ever' , 'also', '?' , '#', '@' ,'(', ')', \"'s\", \"n't\" , '``', \"''\", \n",
    " \"--\", \"cnn\" , \"scrap\" , \"image\"]\n",
    "\n",
    "# get rid of stop words\n",
    "text_no_stops = [[word for word in sentence if word not in stops] for sentence in lemmatized_text]\n",
    "\n",
    "# get rid of punctuation\n",
    "text_no_stops = [[word for word in sentence if word not in string.punctuation] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of works that contain numbers\n",
    "text_no_stops = [[word for word in sentence if not any(char.isdigit() for char in word)] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of single character words\n",
    "text_no_stops = [[word for word in sentence if len(word) > 1] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of contractions\n",
    "text_no_stops = [[word for word in sentence if \"'\" not in word] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of .com\n",
    "text_no_stops = [[word for word in sentence if \".com\" not in word] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of non-printable characters\n",
    "text_no_stops = [[word for word in sentence if word.isprintable()] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of emojis by looking for non-ascii characters\n",
    "text_no_stops = [[word for word in sentence if word.encode('ascii', 'ignore').decode('ascii') == word] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of words that contain - / or . or ^\n",
    "text_no_stops = [[word for word in sentence if '-' not in word] for sentence in text_no_stops]\n",
    "text_no_stops = [[word for word in sentence if '/' not in word] for sentence in text_no_stops]\n",
    "text_no_stops = [[word for word in sentence if '.' not in word] for sentence in text_no_stops]\n",
    "text_no_stops = [[word for word in sentence if '^' not in word] for sentence in text_no_stops]\n",
    "\n",
    "# get rid of duplicate words in each sentence\n",
    "text_no_stops = [list(set(sentence)) for sentence in text_no_stops]\n",
    "\n",
    "text_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for the different topic names to add to the file name\n",
    "topics = ['sports', 'food', 'tech', 'science', 'business', 'politics']\n",
    "\n",
    "for i in range(len(text_no_stops)):\n",
    "    if i + 1 < 10:\n",
    "        if i + 1 == 5:\n",
    "            with open('cleaned_050{}_'.format(i + 1)+(topics[2])+'.txt', 'w') as f:\n",
    "             f.write(' '.join(text_no_stops[i]))\n",
    "        else:\n",
    "            with open('cleaned_050{}_'.format(i + 1)+(topics[3])+'.txt', 'w') as f:\n",
    "             f.write(' '.join(text_no_stops[i]))\n",
    "    else:\n",
    "        if i + 1 == 10 or i + 1 == 15 or i + 1 == 20:\n",
    "            with open('cleaned_05{}_'.format(i + 1)+(topics[2])+'.txt', 'w') as f:\n",
    "             f.write(' '.join(text_no_stops[i]))\n",
    "        else:\n",
    "            with open('cleaned_05{}_'.format(i + 1)+(topics[3])+'.txt', 'w') as f:\n",
    "             f.write(' '.join(text_no_stops[i]))\n",
    "    \n",
    "\n",
    "def save_file_professor(text_no_stops):\n",
    "    for i in range(len(text_no_stops)):\n",
    "        if i + 1 == 1:\n",
    "           with open('cleaned_990{}_'.format(i + 1)+(topics[0])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))\n",
    "        elif i + 1 == 2:\n",
    "           with open('cleaned_990{}_'.format(i + 1)+(topics[1])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))\n",
    "        elif i + 1 == 3:\n",
    "            with open('cleaned_990{}_'.format(i + 1)+(topics[2])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))\n",
    "        elif i + 1 == 4:\n",
    "            with open('cleaned_990{}_'.format(i + 1)+(topics[3])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))\n",
    "        elif i + 1 == 5:\n",
    "            with open('cleaned_990{}_'.format(i + 1)+(topics[4])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))\n",
    "        elif i + 1 == 6:\n",
    "            with open('cleaned_990{}_'.format(i + 1)+(topics[5])+'.txt', 'w') as f:\n",
    "                f.write(' '.join(text_no_stops[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
